{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import division\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import seaborn as sns\n",
    "import random\n",
    "from lin_alg import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sum_of_squares(v):\n",
    "    return sum(v_i ** 2 for v_i in v) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def square(x):\n",
    "    return x * x\n",
    "\n",
    "def derivative(x):\n",
    "    return 2 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def difference_quotient(f, x, h):\n",
    "    return (f(x + h) - f(x)) / h\n",
    "\n",
    "derivative_estimate = partial(difference_quotient, square, h = 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1092eee90>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAegAAAFVCAYAAAAkBHynAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1092eedd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = range(-10, 10)\n",
    "plt.plot(x, map(derivative, x), 'r.', markersize = 15, alpha = 0.5)\n",
    "plt.plot(x, map(derivative_estimate, x), 'b*', markersize = 15, alpha = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partial_difference_quotient(f,  v, i, h):\n",
    "    w = [v_j + (h if j == i else 0)\n",
    "        for j, v_j in enumerate(v)]\n",
    "    \n",
    "    return (f(w) - f(v)) / h\n",
    "\n",
    "def estimate_gradient(f, v, h = 0.00001):\n",
    "    return [partial_difference_quotient(f, v, i, h)\n",
    "           for i, _ in enumerate(v)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step(v, direction, step_size):\n",
    "    return [v_i + step_size * direction_i\n",
    "           for v_i, direction_i in zip(v, direction)]\n",
    "\n",
    "def sum_of_squares_gradient(v):\n",
    "    return [2 * v_i for v_i in v]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# starting pointa\n",
    "v = [random.randint(-10, 10) for i in xrange(3)]\n",
    "\n",
    "tolerance = 0.0000001\n",
    "\n",
    "while True:\n",
    "    graident = sum_of_squares_gradient(v) # gradient at v\n",
    "    next_v = step(v, graident, -0.01) # go down the gradient\n",
    "    if distance(next_v, v) < tolerance: # stop if next step is below tolerance\n",
    "        break\n",
    "    v = next_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.3878671765582534e-06, -2.3131119609304243e-06, 4.163601529674763e-06]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing Step Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def safe(f):\n",
    "    \"\"\"error correction on apply\"\"\"\n",
    "    def safe_f(*args, **kwargs):\n",
    "        try:\n",
    "            return f(*args, **kwargs)\n",
    "        except:\n",
    "            return float('inf')\n",
    "    return safe_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minimize_batch(target_fn, gradient_fn, theta_0, tolerance = 0.000001):\n",
    "    \n",
    "    step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "    \n",
    "    theta = theta_0 # starting location\n",
    "    target_fn = safe(target_fn) # make  target safe\n",
    "    value = target_fn(theta) # this is the value we minimize\n",
    "    \n",
    "    while True:\n",
    "        gradient = gradient_fn(theta) # derivative at current param for ALL points\n",
    "        next_thetas = [step(theta, gradient, -step_size)\n",
    "                      for step_size in step_sizes] # find all possible next params\n",
    "        \n",
    "        # pick theta that minimizes the error function\n",
    "        next_theta = min(next_thetas, key = target_fn)\n",
    "        next_value = target_fn(next_theta)\n",
    "        \n",
    "        # stop if we reach tolerance\n",
    "        if abs(value - next_value) < tolerance:\n",
    "            return theta\n",
    "        else:\n",
    "            theta, value = next_theta, next_value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def negate(f):\n",
    "    return lambda *args, **kwargs: -f(*args, **kwargs)\n",
    "\n",
    "def negate_all(f):\n",
    "    return lambda *args, **kwargs: [-y for y in f(*args, **kwargs)]\n",
    "\n",
    "def maximize_batch(target_fn, gradient_fn, theta_0, tolerance = 0.000001):\n",
    "    return minimize_batch(negate(target_fn),\n",
    "                         negate_all(gradient_fn),\n",
    "                         theta_0,\n",
    "                         tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def in_random_order(data):\n",
    "    indexes = [i for i,_ in enumerate(data)]\n",
    "    random.shuffle(indexes)\n",
    "    \n",
    "    for i in indexes:\n",
    "        yield data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def minmize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0 = 0.01):\n",
    "    \n",
    "    data = zip(x,y)\n",
    "    theta = theta_0\n",
    "    alpha = alpha_0\n",
    "    min_theta, min_value = None, float('inf')\n",
    "    iterations_with_no_improvement = 0\n",
    "    \n",
    "    while iterations_with_no_improvement < 100:\n",
    "        value = sum(target_fn(x_i,y_i,theta) for x_i, y_i in data)\n",
    "        \n",
    "        if value < min_value:\n",
    "            min_theta, min_value = theta, value\n",
    "            iterations_with_no_improvement = 0\n",
    "            alpha = alpha_0\n",
    "        else:\n",
    "            iterations_with_no_improvement += 1\n",
    "            alpha *= 0.9\n",
    "            \n",
    "        # This is the time saving step\n",
    "        for x_i, y_i in in_random_order(data):\n",
    "            gradient_i = gradient_fn(x_i, y_i, theta)\n",
    "            theta = vector_subtract(theta, scalar_multiply(alpha, gradient_i))\n",
    "            \n",
    "    return min_theta\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def maximize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0 = 0.01):\n",
    "    return maximize_stochastic(negate(target_fn),\n",
    "                              negate_all(gradient_fn),\n",
    "                              x, y, theta_0, alpha_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
